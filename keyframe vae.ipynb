{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd3b15e",
   "metadata": {},
   "source": [
    "# Loading Ground truth frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc3ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import cv2\n",
    "\n",
    "# Load the MAT file\n",
    "mat_file = scipy.io.loadmat(\"D:\\College etc\\College\\8th sem\\Project\\SumMe\\GT\\Cooking.mat\")\n",
    "\n",
    "# Extract the segments information from the MAT file\n",
    "segments = mat_file['segments'][0]  # Assuming the segments information is stored as a cell array\n",
    "\n",
    "# Load the video file\n",
    "video = cv2.VideoCapture(\"Cooking.mp4\")\n",
    "\n",
    "# Extract keyframes from each segment\n",
    "keyframes = []\n",
    "for segment in segments:\n",
    "    start_frame = int(segment[0][0])  # Convert start frame index to integer\n",
    "    end_frame = int(segment[0][1])  # Convert end frame index to integer\n",
    "\n",
    "    # Extract keyframes within the segment range\n",
    "    for frame_index in range(start_frame, end_frame + 1):\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_index)  # Set the video frame position to the current frame index\n",
    "        ret, frame = video.read()\n",
    "        if ret:\n",
    "            keyframes.append(frame)\n",
    "\n",
    "# Save the keyframes as individual image files or perform further processing\n",
    "\n",
    "# Release the video file\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d8f4c",
   "metadata": {},
   "source": [
    "## Keyframe extraction for video summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71d5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 99s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "\n",
    "# Load the video file\n",
    "video = cv2.VideoCapture(\"Cooking.mp4\")\n",
    "\n",
    "# Define the VAE model architecture\n",
    "input_shape = (640, 640, 3)\n",
    "latent_dim = 256\n",
    "\n",
    "def vae_model():\n",
    "    input_img = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(latent_dim, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Latent representation\n",
    "    flat = tf.keras.layers.Flatten()(x)\n",
    "    encoded = tf.keras.layers.Dense(latent_dim, activation='relu')(flat)\n",
    "    encoder = Model(input_img, encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    # VAE model\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Create a KMeans clustering object\n",
    "n_clusters = 30\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "# Train the VAE model\n",
    "autoencoder, encoder = vae_model()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Extract frames from the video\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    frames.append(frame)\n",
    "frames = np.array(frames)\n",
    "\n",
    "# Extract the latent representation of the frames\n",
    "encoded_frames = encoder.predict(frames)\n",
    "\n",
    "# Cluster the latent representations using KMeans\n",
    "kmeans.fit(encoded_frames)\n",
    "\n",
    "# Extract the keyframes\n",
    "keyframes = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "    cluster_frames = frames[cluster_indices]\n",
    "    cluster_distances = kmeans.transform(encoded_frames[cluster_indices])\n",
    "    keyframe_index = np.argmin(np.max(cluster_distances, axis=1))\n",
    "    keyframe = cluster_frames[keyframe_index]\n",
    "    keyframes.append(keyframe)\n",
    "\n",
    "# Save the keyframes as an MP4 video\n",
    "height, width, layers = keyframes[0].shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter('keyframes_cook.mp4', fourcc, 30, (width, height))\n",
    "for frame in keyframes:\n",
    "    video_writer.write(frame)\n",
    "video_writer.release()\n",
    "\n",
    "# Release the video file\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c71999",
   "metadata": {},
   "source": [
    "## Calcluating SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66adaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.9771203464670698\n",
      "SSIM: 0.981167755327745\n",
      "SSIM: 0.9766605533964451\n",
      "SSIM: 0.9828939196569944\n",
      "SSIM: 0.9843180626916304\n",
      "SSIM: 0.9805899616481492\n",
      "SSIM: 0.9797184550289161\n",
      "SSIM: 0.9796338490729566\n",
      "SSIM: 0.9783998935257417\n",
      "SSIM: 0.9795186360366318\n",
      "SSIM: 0.9812017031879224\n",
      "SSIM: 0.9808075843688635\n",
      "SSIM: 0.9846464992743256\n",
      "SSIM: 0.9807443122321227\n",
      "SSIM: 0.9816415996355486\n",
      "SSIM: 0.9810423921162751\n",
      "SSIM: 0.9824884913287779\n",
      "SSIM: 0.9816452474669005\n",
      "SSIM: 0.9804515196059042\n",
      "SSIM: 0.9818186394673101\n",
      "SSIM: 0.9798226403418732\n",
      "SSIM: 0.9827327171383569\n",
      "SSIM: 0.9808836308997407\n",
      "SSIM: 0.9809050682591994\n",
      "SSIM: 0.9811758232375432\n",
      "SSIM: 0.9769646041582646\n",
      "SSIM: 0.9813404210694728\n",
      "SSIM: 0.978958284637155\n",
      "SSIM: 0.9794789723234867\n",
      "SSIM: 0.9787712802436588\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Load the extracted keyframes from your code\n",
    "extracted_keyframes = []\n",
    "\n",
    "video = cv2.VideoCapture('keyframes_cook.mp4')\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    extracted_keyframes.append(frame)\n",
    "video.release()\n",
    "\n",
    "# Load the ground truth keyframes from your code\n",
    "ground_truth_keyframes = keyframes  # Replace 'keyframes' with the variable name storing the ground truth keyframes\n",
    "\n",
    "# Compare each keyframe\n",
    "for extracted_frame, ground_truth_frame in zip(extracted_keyframes, ground_truth_keyframes):\n",
    "    # Convert frames to grayscale for SSIM comparison\n",
    "    extracted_frame_gray = cv2.cvtColor(extracted_frame, cv2.COLOR_BGR2GRAY)\n",
    "    ground_truth_frame_gray = cv2.cvtColor(ground_truth_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate SSIM between keyframes\n",
    "    similarity = ssim(extracted_frame_gray, ground_truth_frame_gray)\n",
    "    \n",
    "    # Print the similarity value\n",
    "    print(f\"SSIM: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Load the extracted keyframe video\n",
    "video = cv2.VideoCapture('cooking_keyframes.mp4')\n",
    "\n",
    "# Extract the keyframes from the video\n",
    "extracted_keyframes = []\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    extracted_keyframes.append(frame)\n",
    "video.release()\n",
    "\n",
    "# Load the ground truth keyframes from your code\n",
    "ground_truth_keyframes = keyframes  # Replace 'keyframes' with the variable name storing the ground truth keyframes\n",
    "\n",
    "# Define the threshold for similarity\n",
    "threshold = 0.9\n",
    "\n",
    "# Calculate the number of true positives, false positives, and false negatives\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "# Compare each keyframe\n",
    "for extracted_frame in extracted_keyframes:\n",
    "    # Convert frames to grayscale for SSIM comparison\n",
    "    extracted_frame_gray = cv2.cvtColor(extracted_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the maximum similarity with ground truth keyframes\n",
    "    max_similarity = 0.0\n",
    "    for ground_truth_frame in ground_truth_keyframes:\n",
    "        ground_truth_frame_gray = cv2.cvtColor(ground_truth_frame, cv2.COLOR_BGR2GRAY)\n",
    "        similarity = ssim(extracted_frame_gray, ground_truth_frame_gray)\n",
    "        max_similarity = max(max_similarity, similarity)\n",
    "    \n",
    "    # Determine if it's a true positive or false positive based on the threshold\n",
    "    if max_similarity >= threshold:\n",
    "        true_positives += 1\n",
    "    else:\n",
    "        false_positives += 1\n",
    "\n",
    "# Calculate the number of false negatives\n",
    "false_negatives = len(ground_truth_keyframes) - true_positives\n",
    "\n",
    "# Calculate precision, recall, and F-measure\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f_measure = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-measure: {f_measure}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
